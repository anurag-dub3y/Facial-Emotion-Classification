{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport numpy as np\nfrom collections import OrderedDict\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras import backend as K\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = \"../input/ck48-5-emotions/CK+48/\"\n\ntotal_images = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    count = 0\n    for f in os.listdir(INPUT_PATH + dir_ + \"/\"):\n        count += 1\n    total_images += count\n    print(f\"{dir_} has {count} number of images\")\n    \nprint(f\"\\ntotal images: {total_images}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`sadness` and `fear` has very low number of images as compared to other classes","metadata":{}},{"cell_type":"code","source":"TOP_EMOTIONS = [\"happy\", \"surprise\", \"anger\", \"sadness\", \"fear\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_arr = np.empty(shape=(total_images, 48, 48, 1))\nimg_label = np.empty(shape=(total_images))\nlabel_to_text = {}\n\nidx = 0\nlabel = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    if dir_ in  TOP_EMOTIONS:\n        for f in os.listdir(INPUT_PATH + dir_ + \"/\"):\n            img_arr[idx] = np.expand_dims(cv2.imread(INPUT_PATH + dir_ + \"/\" + f, 0), axis=2)\n            img_label[idx] = label\n            idx += 1\n        label_to_text[label] = dir_\n        label += 1\n\nimg_label = np_utils.to_categorical(img_label)\n\nimg_arr.shape, img_label.shape, label_to_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(img_arr, img_label, train_size=0.7, stratify=img_label, shuffle=True, random_state=42)\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = pyplot.figure(1, (10,10))\n\nidx = 0\nfor k in label_to_text:\n    sample_indices = np.random.choice(np.where(y_train[:,k]==1)[0], size=5, replace=False)\n    sample_images = X_train[sample_indices]\n    for img in sample_images:\n        idx += 1\n        ax = pyplot.subplot(5,5,idx)\n        ax.imshow(img.reshape(48,48), cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(label_to_text[k])\n        pyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data normalization\nX_train = X_train / 255.\nX_test = X_test / 255.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dcnn(input_shape, num_classes):\n    model_in = Input(shape=input_shape, name=\"input\")\n    \n    conv2d_1 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_1'\n    )(model_in)\n    batchnorm_1 = BatchNormalization(name='batchnorm_1')(conv2d_1)\n    conv2d_2 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_2'\n    )(batchnorm_1)\n    batchnorm_2 = BatchNormalization(name='batchnorm_2')(conv2d_2)\n    \n    maxpool2d_1 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_1')(batchnorm_2)\n    dropout_1 = Dropout(0.3, name='dropout_1')(maxpool2d_1)\n\n    conv2d_3 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_3'\n    )(dropout_1)\n    batchnorm_3 = BatchNormalization(name='batchnorm_3')(conv2d_3)\n    conv2d_4 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_4'\n    )(batchnorm_3)\n    batchnorm_4 = BatchNormalization(name='batchnorm_4')(conv2d_4)\n    \n    maxpool2d_2 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_2')(batchnorm_4)\n    dropout_2 = Dropout(0.3, name='dropout_2')(maxpool2d_2)\n\n    conv2d_5 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_5'\n    )(dropout_2)\n    batchnorm_5 = BatchNormalization(name='batchnorm_5')(conv2d_5)\n    conv2d_6 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_6'\n    )(batchnorm_5)\n    batchnorm_6 = BatchNormalization(name='batchnorm_6')(conv2d_6)\n    \n    maxpool2d_3 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_3')(batchnorm_6)\n    dropout_3 = Dropout(0.3, name='dropout_3')(maxpool2d_3)\n\n    flatten = Flatten(name='flatten')(dropout_3)\n    \n    dense_1 = Dense(\n        128,\n        activation='elu',\n        kernel_initializer='he_normal',\n        name='dense1'\n    )(flatten)\n    batchnorm_7 = BatchNormalization(name='batchnorm_7')(dense_1)\n    dropout_4 = Dropout(0.4, name='dropout_4')(batchnorm_7)\n\n    model_out = Dense(\n        num_classes,\n        activation='softmax',\n        name='out_layer'\n    )(dropout_4)\n\n    model = Model(inputs=model_in, outputs=model_out, name=\"DCNN\")\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_SHAPE = (48, 48, 1)\noptim = optimizers.Adam(0.001)\n\nmodel = build_dcnn(input_shape=(48,48,1), num_classes=len(label_to_text))\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n)\n\nplot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='model.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00008,\n    patience=12,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    factor=0.4,\n    patience=6,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]\n\nbatch_size = 10\nepochs = 60","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n)\ntrain_datagen.fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_test, y_test),\n    steps_per_epoch=len(X_train) / batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history.png')\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_to_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_to_label = dict((v,k) for k,v in label_to_text.items())\ntext_to_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat_test = model.predict(X_test)\nyhat_test = np.argmax(yhat_test, axis=1)\nytest_ = np.argmax(y_test, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(ytest_, yhat_test, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_model3pipes.png\")\n\ntest_accu = np.sum(ytest_ == yhat_test) / len(ytest_) * 100\nprint(f\"test accuracy: {round(test_accu, 4)} %\\n\\n\")\n\nprint(classification_report(ytest_, yhat_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_list = [(layer.name, layer) for layer in model.layers if \"conv\" in layer.name]\nlayer_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's Visualize What our CNN Learned","metadata":{}},{"cell_type":"markdown","source":"#### Plot Filters","metadata":{}},{"cell_type":"code","source":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_4\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nidx = 1\nfor layer in layer_list:\n    if layer[0] in INTERESTED_CONV_LAYERS:        \n        layer_output = layer[1].output\n        filters, bias = layer[1].get_weights()     \n        filters = (filters - filters.min()) / (filters.max() - filters.min())\n    \n        cols = 20\n        rows = math.ceil(filters.shape[-1] / cols)\n        fig = pyplot.figure(i, (20, rows))\n\n        idx += 1\n        for i,f in enumerate(np.rollaxis(filters, 3)):\n            ax = pyplot.subplot(rows, cols, i+1)\n            f = np.mean(f, axis=2)\n            ax.imshow(f, cmap=\"viridis\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.suptitle(f\"layer name: {layer[0]}, {filters.shape[3]} filters of shape {filters.shape[:-1]}\", fontsize=20, y=1.1)\n            pyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot Feature Maps","metadata":{}},{"cell_type":"code","source":"sns.reset_orig()\nsample_img = X_test[0] # select a random image\npyplot.imshow(sample_img.reshape(48,48), cmap=\"gray\")\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img = np.expand_dims(sample_img, axis=0)\nsample_img.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_4\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ni = 1\nfor layer in layer_list:\n    if layer[0] in INTERESTED_CONV_LAYERS:    \n        model_conv2d = Model(inputs=model.inputs, outputs=layer[1].output)\n        featuremaps_conv2d = model_conv2d.predict(sample_img)\n\n        cols = 20\n        rows = math.ceil(featuremaps_conv2d.shape[-1] / cols)\n        fig = pyplot.figure(i, (20, rows))        \n        i += 1\n        \n        for idx, feature_map in enumerate(np.rollaxis(featuremaps_conv2d, axis=3)):\n            ax = pyplot.subplot(rows, cols ,idx+1)\n            ax.imshow(feature_map[0], cmap=\"viridis\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.suptitle(f\"layer name: {layer[0]}, feature map shape: {featuremaps_conv2d.shape}\", fontsize=20, y=1.1)\n            pyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the first convolutional layer has detected many edges in the image and indeed this is what we expected.","metadata":{}},{"cell_type":"markdown","source":"#### Plot Class Activation Map (CAM) \nThe following CAM is taken from [here](https://github.com/himanshurawlani/convnet-interpretability-keras/blob/master/Visualizing%20heatmaps/heatmap_visualization_using_gradcam.ipynb).","metadata":{}},{"cell_type":"code","source":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_2\", \"conv2d_3\", \"conv2d_4\", \"conv2d_5\", \"conv2d_6\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(sample_img)\nlabel_to_text[np.argmax(preds[0])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_vector_output = model.output[:, np.argmax(preds[0])]\npred_vector_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heatmaps = []\n\nfor layer in layer_list:\n    if layer[0] in INTERESTED_CONV_LAYERS:\n        some_conv_layer = model.get_layer(layer[0])\n        grads = K.gradients(pred_vector_output, some_conv_layer.output)[0]\n        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n        iterate = K.function([model.input], [pooled_grads, some_conv_layer.output[0]])\n        pooled_grads_value, conv_layer_output_value = iterate([sample_img])\n\n        for i in range(model.get_layer(layer[0]).output_shape[-1]):\n            conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n        heatmaps.append(np.mean(conv_layer_output_value, axis=-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = pyplot.figure(figsize=(14, 3))\n\nfor i, (name,hm) in enumerate(zip(INTERESTED_CONV_LAYERS, heatmaps)):\n    ax = pyplot.subplot(1, 6, i+1)\n    img_heatmap = np.maximum(hm, 0)\n    img_heatmap /= np.max(img_heatmap)\n    ax.imshow(img_heatmap, cmap=\"viridis\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    pyplot.title(name)\n    pyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our model focuses on important aspects of the image i.e., `lips`, `eyes` and `eyebrows`.","metadata":{}},{"cell_type":"code","source":"fig = pyplot.figure(figsize=(14, 3))\n\nfor i, (name,hm) in enumerate(zip(INTERESTED_CONV_LAYERS, heatmaps)):\n    img_heatmap = np.maximum(hm, 0)\n    img_heatmap /= np.max(img_heatmap)\n    \n    img_hm = cv2.resize(img_heatmap, (48,48))\n    img_hm = np.uint8(255 * img_hm)\n\n    img_hm = cv2.applyColorMap(img_hm, cv2.COLORMAP_JET)\n\n    # 0.4 here is a heatmap intensity factor\n    superimposed_img = img_hm * 0.4 + sample_img\n\n    ax = pyplot.subplot(1, 6, i+1)\n    ax.imshow(superimposed_img[0,:,:])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    pyplot.title(name)\n    pyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Infact this all can be done just using few lines of code using some interesting libraries like `keract`, `keras-viz`, `lucid`. Below I showed keract","metadata":{}},{"cell_type":"code","source":"! pip install keract","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keract import get_activations, display_activations, display_heatmaps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activations = get_activations(model, sample_img)\nactivations.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As there are many layers in the model and visualizing all of them is not very informative, so I just plotted the interested ones i.e., conv layers","metadata":{}},{"cell_type":"code","source":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_2\", \"conv2d_3\", \"conv2d_4\", \"conv2d_5\", \"conv2d_6\", \"dense1\", \"out_layer\"]\n\nactivations_convs = OrderedDict()\n\nfor k,v in activations.items():\n    if k in INTERESTED_CONV_LAYERS:\n        activations_convs[k] = v\n        \nactivations_convs.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_activations(activations_convs, save=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_heatmaps(activations_convs, sample_img, save=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
